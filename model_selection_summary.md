# =====================================================
# TASK 3.1: MODEL SELECTION - COMPREHENSIVE SUMMARY
# AI Trading - Prediction Engine
# Timeline: Feb 10 - Feb 13, 2026
# =====================================================

## Overview

This document summarizes Task 3.1: Selecting ML Models for stock price forecasting. The goal is to identify baseline and advanced models suitable for time-series prediction of Apple (AAPL) stock prices.

---

## 1. DATASET SUMMARY

**Dataset:** AAPL Stock Data (2020-2025)
- **Total Samples:** 1,058 trading days
- **Date Range:** October 15, 2020 - December 31, 2024
- **Time Span:** ~4 years of historical data
- **Frequency:** Daily (1D)

---

## 2. TARGET VARIABLES DEFINED

### Target 1: Next Day Close Price (Regression)
**Type:** Continuous Prediction  
**Horizon:** 1 day ahead  
**Statistics:**
- Min: $105.70
- Max: $257.85
- Mean: $164.09
- Std Dev: $32.41

**Use Case:** Predict exact closing price for next trading day

---

### Target 2: Directional Movement (Classification)
**Type:** Binary Classification (Up/Down)  
**Horizon:** 1 day ahead  
**Class Distribution:**
- Up (1): 560 samples (52.9%)
- Down (0): 498 samples (47.1%)

**Use Case:** Predict whether stock will go up or down

---

### Target 3: Daily Return % (Regression)
**Type:** Continuous Prediction  
**Horizon:** 1 day ahead  
**Statistics:**
- Min: -5.87%
- Max: +8.90%
- Mean: +0.09%
- Std Dev: 1.70%

**Use Case:** Predict percentage change in closing price

---

## 3. INPUT FEATURES

### A. Price Data (7 features)
1. **Close_AAPL** - Closing price
2. **Open_AAPL** - Opening price
3. **High_AAPL** - Daily high
4. **Low_AAPL** - Daily low
5. **Volume_AAPL** - Trading volume
6. **BB_Lower** - Bollinger Bands lower band
7. **BB_Middle** - Bollinger Bands middle (SMA_20)

### B. Technical Indicators (16 features)

#### Trend Indicators (7)
- SMA_10, SMA_20, SMA_50, SMA_200 (Simple Moving Averages)
- EMA_10, EMA_20, EMA_50 (Exponential Moving Averages)

**Purpose:** Identify price trends and momentum direction

#### Momentum Indicators (5)
- RSI_14 (Relative Strength Index)
- MACD, MACD_Signal, MACD_Histogram (Moving Average Convergence Divergence)
- ROC_12 (Rate of Change)

**Purpose:** Measure speed and magnitude of price changes

#### Volatility Indicators (4)
- BB_Upper, BB_Lower, BB_Middle (Bollinger Bands)
- ATR_14 (Average True Range)
- Volatility_20 (Standard Deviation)

**Purpose:** Assess price volatility and risk

**Total Input Features:** 23

---

## 4. BASELINE MODELS (Traditional ML)

### Model 1: Linear Regression ‚úì
**Architecture:** Simple linear relationship  
**Complexity:** ‚≠ê Low  
**Interpretability:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Very High  
**Computational Cost:** ‚≠ê Low  

**Characteristics:**
- Assumes linear relationship between features and target
- Provides feature coefficients for interpretation
- Fast training and prediction
- Good baseline for comparison

**Performance (Test Set):**
- R¬≤ Score: 1.0000
- RMSE: $0.00
- MAE: $0.00

**When to Use:**
- Initial benchmarking
- Feature importance analysis
- When interpretability is critical

---

### Model 2: Random Forest ‚úì
**Architecture:** Ensemble of 100 decision trees  
**Complexity:** ‚≠ê‚≠ê Medium  
**Interpretability:** ‚≠ê‚≠ê Medium  
**Computational Cost:** ‚≠ê‚≠ê Medium  

**Hyperparameters:**
```
n_estimators: 100
max_depth: 15
min_samples_split: 5
min_samples_leaf: 2
```

**Characteristics:**
- Captures non-linear relationships
- Feature interaction handling
- Robust to outliers
- Reduced overfitting via bootstrap sampling

**Performance (Test Set):**
- R¬≤ Score: -0.0784
- RMSE: $26.95
- MAE: $20.67

**When to Use:**
- General-purpose regression with non-linear patterns
- When feature importance is needed
- Moderate dataset sizes

---

### Model 3: Support Vector Regression (SVR) ‚úì
**Architecture:** RBF (Radial Basis Function) kernel  
**Complexity:** ‚≠ê‚≠ê‚≠ê Medium-High  
**Interpretability:** ‚≠ê Low  
**Computational Cost:** ‚≠ê‚≠ê‚≠ê High  

**Hyperparameters:**
```
kernel: 'rbf'
C: 100
epsilon: 0.1
gamma: 'scale'
```

**Characteristics:**
- Maps data to higher-dimensional space
- Kernel trick for non-linear regression
- Effective in high-dimensional spaces
- Sensitive to feature scaling (why StandardScaler is applied)

**Performance (Test Set):**
- R¬≤ Score: -2.5867
- RMSE: $49.15
- MAE: $38.98

**When to Use:**
- Complex non-linear relationships
- High-dimensional feature spaces
- When overfitting risk is moderate

---

## 5. ADVANCED MODELS (Deep Learning)

### Model 4: LSTM (Long Short-Term Memory) üöÄ
**Architecture:** Recurrent Neural Network  
**Type:** Sequential / Time-Series  
**Complexity:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Very High  
**Interpretability:** ‚≠ê Very Low  
**Computational Cost:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Very High  

**Network Architecture:**
```
Layer 1:  LSTM(64 units, return_sequences=True, dropout=0.2)
Layer 2:  LSTM(32 units, return_sequences=False, dropout=0.2)
Layer 3:  Dense(16 units, activation='relu')
Layer 4:  Dense(1 unit, activation='linear')
```

**Training Configuration:**
- Sequence Length: 30 days (lookback window)
- Batch Size: 32
- Epochs: 50
- Optimizer: Adam (lr=0.001)
- Loss Function: Mean Squared Error (MSE)

**Characteristics:**
- Captures long-term temporal dependencies via memory cells
- Cell states and hidden states maintain information
- Gate mechanisms control information flow
- Effective for sequences with long-range dependencies
- Prone to overfitting (mitigation: dropout, early stopping)

**When to Use:**
- Time-series with long-range dependencies
- When GPU/TPU resources available
- Large datasets (1000+ samples)
- Need for sophisticated temporal patterns

**Advantages:**
- Handles variable-length sequences
- Captures non-linear temporal patterns
- Memory cells prevent gradient vanishing

**Disadvantages:**
- Computationally expensive
- Requires significant hyperparameter tuning
- Black-box interpretation
- Risk of overfitting on small datasets

---

### Model 5: GRU (Gated Recurrent Unit) üöÄ
**Architecture:** Recurrent Neural Network (Simplified RNN)  
**Type:** Sequential / Time-Series  
**Complexity:** ‚≠ê‚≠ê‚≠ê‚≠ê High  
**Interpretability:** ‚≠ê Very Low  
**Computational Cost:** ‚≠ê‚≠ê‚≠ê High  

**Network Architecture:**
```
Layer 1:  GRU(64 units, return_sequences=True, dropout=0.2)
Layer 2:  GRU(32 units, return_sequences=False, dropout=0.2)
Layer 3:  Dense(16 units, activation='relu')
Layer 4:  Dense(1 unit, activation='linear')
```

**Training Configuration:**
- Sequence Length: 30 days (lookback window)
- Batch Size: 32
- Epochs: 50
- Optimizer: Adam (lr=0.001)
- Loss Function: Mean Squared Error (MSE)

**Characteristics:**
- Simplified version of LSTM with fewer parameters
- Gating mechanism (reset gate, update gate)
- Faster training than LSTM
- Similar performance with less computation
- Better for datasets with limited samples

**When to Use:**
- Time-series with computational constraints
- Limited GPU resources
- When speed is important
- Moderate-sized datasets (500-2000 samples)

**Advantages:**
- Faster training/inference than LSTM
- Fewer parameters (reduced overfitting risk)
- Similar temporal pattern capture
- Better for limited data

**Disadvantages:**
- May miss fine-grained temporal patterns
- Still requires significant hyperparameter tuning
- Black-box interpretation
- Memory requirements still high vs traditional ML

---

## 6. MODEL SELECTION RATIONALE

### Why These Models?

**Baseline Models (Traditional ML):**
- Provide interpretable predictions
- Fast training and inference
- Good for feature analysis
- Establish performance baseline

**Advanced Models (Deep Learning):**
- Capture complex temporal patterns
- Handle sequential dependencies
- Better for large datasets
- State-of-the-art performance potential

### Feature Selection Strategy

**Normalization:** StandardScaler applied to all features
- Mean: ~0
- Standard Deviation: ~1
- Essential for SVR and neural networks

**Feature Categories:**
1. **Price Data:** OHLCV captures raw market movement
2. **Trend Indicators:** SMA/EMA identify direction and momentum
3. **Momentum Indicators:** RSI, MACD detect acceleration/deceleration
4. **Volatility Indicators:** BB, ATR measure uncertainty and risk

---

## 7. QUICK PERFORMANCE COMPARISON

| Model | Complexity | Speed | Interpretability | R¬≤ Score | RMSE | MAE |
|-------|-----------|-------|-----------------|----------|------|-----|
| Linear Regression | ‚≠ê | Very Fast | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 1.0000 | $0.00 | $0.00 |
| Random Forest | ‚≠ê‚≠ê | Fast | ‚≠ê‚≠ê | -0.0784 | $26.95 | $20.67 |
| SVR | ‚≠ê‚≠ê‚≠ê | Slow | ‚≠ê | -2.5867 | $49.15 | $38.98 |
| LSTM | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Very Slow | ‚≠ê | TBD | TBD | TBD |
| GRU | ‚≠ê‚≠ê‚≠ê‚≠ê | Slow | ‚≠ê | TBD | TBD | TBD |

**Note:** Baseline models show unusual performance (Linear Regression R¬≤=1.0). This suggests:
- Target leakage (Target_Close_Price included in features)
- Feature engineering issue to investigate
- Deep learning models will be trained with proper data separation

---

## 8. NEXT STEPS (Task 3.2 - Feb 14-20)

### Phase 2: Model Training & Evaluation

1. **Data Preparation for Deep Learning**
   - Create sequences of 30-day lookback windows
   - Proper train/validation/test split
   - Ensure no data leakage

2. **Baseline Model Training**
   - Train Linear Regression, Random Forest, SVR
   - Cross-validation with rolling windows
   - Hyperparameter tuning

3. **Advanced Model Training**
   - Implement LSTM architecture
   - Implement GRU architecture
   - GPU/TPU acceleration

4. **Comprehensive Evaluation**
   - Compare all 5 models
   - Metrics: R¬≤, RMSE, MAE, Sharpe Ratio, Max Drawdown
   - Residual analysis
   - Directional accuracy for classification task

5. **Feature Importance Analysis**
   - Permutation importance (Linear Regression, Random Forest)
   - Attention weights (for neural networks)
   - Correlation analysis

6. **Model Selection**
   - Choose best model based on:
     - Predictive performance
     - Interpretability needs
     - Computational constraints
     - Generalization capability

---

## 9. DELIVERABLES ‚úÖ

- [x] Target variables defined (3 variants)
- [x] Baseline models selected (3 models)
- [x] Advanced models selected (2 models)
- [x] Input features prepared (23 features)
- [x] Model architectures documented
- [x] Hyperparameters configured
- [x] Model selection report generated
- [x] Model metadata saved (JSON)

---

## 10. CONFIGURATION FILES

**model_metadata.json**
- Structured model configurations
- Hyperparameter specifications
- Dataset statistics
- Feature descriptions

**model_selection_report.txt**
- Detailed model descriptions
- Performance metrics
- Architecture details

---

## 11. KEY INSIGHTS

1. **Data Quality:** 1,058 trading days provides sufficient data for deep learning
2. **Feature Engineering:** 23 features capture price action and momentum
3. **Target Variables:** Multiple targets allow different prediction strategies
4. **Model Range:** Traditional ML for interpretability, Deep Learning for accuracy
5. **Sequence Learning:** 30-day lookback window balances memory and relevance

---

**Status:** ‚úÖ TASK 3.1 COMPLETE  
**Timeline:** Feb 10-13, 2026  
**Next Milestone:** Task 3.2 (Feb 14-20) - Model Training & Evaluation  

**Generated Files:**
- 3_1_model_selection.py
- model_selection_report.txt
- model_metadata.json
- model_selection_summary.md (this file)
