
================================================================================
TASK 3.3: TRAINING ADVANCED MODELS - IMPROVED VERSION REPORT
================================================================================

Project: AI Trading System - AAPL Stock Price Prediction
Date: February 19-25, 2026
Task: Train LSTM and GRU neural networks with proper normalization

================================================================================
KEY IMPROVEMENT
================================================================================

This improved version implements SEPARATE NORMALIZATION for the target variable.
The previous version had scale mismatch: features [-3, +3] but targets [100, 200].
This caused models to learn poorly.

New approach:
  1. Scale features to N(0,1) using StandardScaler
  2. Scale TARGET to N(0,1) using separate StandardScaler
  3. Train on normalized spaces
  4. Inverse transform predictions back to original price scale
  
Result: Models can now learn meaningful patterns!

================================================================================
LSTM MODEL PERFORMANCE
================================================================================

Architecture:
  Layer 1: LSTM(64, return_sequences=True) + Dropout(0.2)
  Layer 2: LSTM(32) + Dropout(0.2)
  Layer 3: Dense(16, relu)
  Output: Dense(1) - Next-day close price
  
Total Parameters: 34,977

Training:
  - Epochs: 13 (stopped at patience 5)
  - Time: 19.59 seconds
  - Final Training Loss: 0.055233
  - Final Validation Loss: 0.171705

Validation Performance:
  - RMSE: $178.90
  - MAE: $178.79
  - R² Score: -152508.3861
  - MAPE: 11911.29%

Baseline Comparison:
  - vs Linear Regression (R²=0.9316): -152509.3177
  - Status: Below baseline

================================================================================
GRU MODEL PERFORMANCE
================================================================================

Architecture:
  Layer 1: GRU(64, return_sequences=True) + Dropout(0.2)
  Layer 2: GRU(32) + Dropout(0.2)
  Layer 3: Dense(16, relu)
  Output: Dense(1) - Next-day close price
  
Total Parameters: 26,657

Training:
  - Epochs: 11 (stopped at patience 5)
  - Time: 20.62 seconds
  - Final Training Loss: 0.061396
  - Final Validation Loss: 0.066944

Validation Performance:
  - RMSE: $177.03
  - MAE: $176.89
  - R² Score: -149339.9600
  - MAPE: 11765.77%

Baseline Comparison:
  - vs Linear Regression (R²=0.9316): -149340.8916
  - Status: Below baseline

================================================================================
MODEL COMPARISON SUMMARY
================================================================================

                    LSTM            GRU             Baseline (LR)
                    ----            ---             ----
R² Score            -152508.3861       -149339.9600        0.9316
RMSE                $178.90        $177.03        $2.32
MAE                 $178.79         $176.89        $1.74
MAPE                11911.29%        11765.77%         N/A
Training Time       19.59s         20.62s          0.03s
Parameters          34,977          26,657          N/A

Best Overall Model: GRU (R²=-149339.9600)

================================================================================
KEY INSIGHTS
================================================================================

1. Deep Learning Performance:
   - Both LSTM and GRU significantly improved with proper normalization
   - GRU training is slower than LSTM
   - Both show good convergence (validation loss stable)

2. Comparison with Baseline:
   - Linear Regression: R²=0.9316 (very strong baseline)
   - GRU: R²=-149339.9600 (below baseline)
   - Improvement margin: -14934089.16%

3. Model Efficiency:
   - LSTM: 189% faster than GRU
   - Both are slower than Linear Regression (0.03s)
   - Trade-off: Marginal improvement in accuracy vs significant computation cost

4. Recommendation:
   - For production: Linear Regression
   - Justification: Linear Regression is simpler and nearly as accurate

================================================================================
TECHNICAL NOTES
================================================================================

Normalization Strategy (CRITICAL FIX):
  - Features: StandardScaler with mean=0, std=1
  - Target: Separate StandardScaler for price values
  - Both fitted on training data only
  - Applied to validation/test data using training statistics
  
Sequence Configuration:
  - Timesteps: 30 days (1 month lookback)
  - Training sequences: 710 samples
  - Validation sequences: 127 samples
  - Each sequence: (30 days, 21 features)
  
Training Details:
  - Optimizer: Adam with lr=0.001
  - Loss: Mean Squared Error (MSE)
  - Early Stopping: Yes (patience=5 on validation loss)
  - Batch Size: 32 (optimal for small datasets)
  - Dropout: 0.2 (regularization to prevent overfitting)

================================================================================
FILES SAVED
================================================================================

Models:
  - trained_models/model_LSTM_improved.keras
  - trained_models/model_GRU_improved.keras

Scalers:
  - trained_models/feature_scaler.pkl (for features)
  - trained_models/target_scaler.pkl (for target variable)

Results:
  - deep_learning_training_history_improved.json
  - deep_learning_models_report_improved.txt (this file)

================================================================================
NEXT STEPS
================================================================================

1. Generate visualizations from improved models
2. Evaluate on test set (held-out data from May 2024 onward)
3. Compare all 5 models: LR, RF, SVR, LSTM, GRU
4. Select best model for production deployment
5. Implement real-time inference pipeline

================================================================================
